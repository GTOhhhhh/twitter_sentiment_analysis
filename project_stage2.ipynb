{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis: Classifying Hate Speech (Pt II)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal for this project is to build a model that will allow us to detect hate speech in tweets. In this case a tweet is classified as hate speech if it contains racist or sexist remarks.\n",
    "\n",
    "All data for this project is sourced from: https://datahack.analyticsvidhya.com/contest/practice-problem-twitter-sentiment-analysis/. Inspiration and some code for this project has been taken from:\n",
    "- 'Stemming - Natural Language Processing With Python and NLTK p.3'- https://www.youtube.com/watch?v=yGKTphqxR9Q\n",
    "- https://datascienceplus.com/twitter-analysis-with-python/\n",
    "- Ultimate guide to deal with Text Data (using Python) – for Data Scientists & Engineers https://www.analyticsvidhya.com/blog/2018/02/the-different-methods-deal-text-data-predictive-python/\n",
    "- 'Deep Learning tutorials in jupyter notebooks.' - https://github.com/sachinruk/deepschool.io\n",
    "- 'Text Classification Using CNN, LSTM and visualize Word Embeddings' - https://medium.com/@sabber/classifying-yelp-review-comments-using-cnn-lstm-and-visualize-word-embeddings-part-2-ca137a42a97d\n",
    "- Tensor Flow: Embeddings Guide - https://www.tensorflow.org/guide/embedding\n",
    "- Understanding LSTM Networks - http://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Setup ##\n",
    "The data set we will be using to train our model consists of 31962 labelled tweets. Each row is 3-tuple (id, label, tweet). A label of 1 means the tweet is hate speech, a label of 0 is a normal tweet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0   @user when a father is dysfunctional and is s...\n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3      0                                bihday your majesty\n",
       "3   4      0  #model   i love u take with u all the time in ...\n",
       "4   5      0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from textblob import Word\n",
    "\n",
    "df = pd.read_csv('./train_E6oV3lV.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31962, 3)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to prepare the data using the data cleaning techniques implemented in stage 1 as well as a few new steps. These include:\n",
    "- transform all tweet text to be lower case\n",
    "- remove punctuation\n",
    "- clean white space\n",
    "- remove urls\n",
    "- lemmatization\n",
    "- \n",
    "For a full explanation of these techniques & their applicablity to improving the classification task please refer to the notebook from part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform to lower case\n",
    "df.tweet = df.tweet.apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "df.tweet.head()\n",
    "\n",
    "# remove punctuation\n",
    "df.tweet = df.tweet.str.replace('[^\\w\\s]','')\n",
    "\n",
    "# remove urls\n",
    "df.tweet = df.tweet.str.replace(r'http[\\w:/\\.]+','') \n",
    "\n",
    "df.tweet = df.tweet.str.replace(r'\\.\\.+','.') #replace multple periods with a single one\n",
    "df.tweet = df.tweet.str.replace(r'\\.',' .')   #replace multple periods with a single one\n",
    "df.tweet = df.tweet.str.replace(r'\\s\\s+',' ') #replace multple white space with a single one\n",
    "\n",
    "\n",
    "# lemmatization\n",
    "df.tweet = df.tweet.apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "df.tweet = df.tweet.str.strip() \n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To quantify the reliability of our classification model we will use the **F1 Score** described below:\n",
    "\n",
    "**True Positives (TP)** - These are the correctly predicted positive values which means that the value of actual class is yes and the value of predicted class is also yes.\n",
    "\n",
    "**True Negatives (TN)** - These are the correctly predicted negative values which means that the value of actual class is no and value of predicted class is also no.\n",
    "\n",
    "**False Positives (FP)** – When actual class is no and predicted class is yes.\n",
    "\n",
    "**False Negatives (FN)** – When actual class is yes but predicted class in no.\n",
    "\n",
    "**Precision** = TP/TP+FP\n",
    "\n",
    "**Recall** = TP/TP+FN\n",
    "\n",
    "**F1 Score** = 2*(Recall * Precision) / (Recall + Precision)\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "\n",
    "![title](precision_recall.png) https://en.wikipedia.org/wiki/F1_score\n",
    "\n",
    "### check this bad boy out for figuring out evaluations (from \n",
    "http://scikit-learn.org/stable/modules/model_evaluation.html#model-evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Approach ##\n",
    "In Stage 1 of our sentiment analysis project we built a simple model by counting the frequencies of every word as they occur in each tweet. This is known as the 'bag-of-words' (BoW) method in in Natural Language rocessing (NLP). We can think of the bag-of-words as representing each tweet as a multiset, or alternatively a vector. While this feature is relatively simple to extract, for stage 2 we will employ a more sophisticated approach using a specific type of Recurring Neural Net (RNN) known as a Long Short Term Memory network (LSTM). \n",
    "\n",
    "Compared with the 'bag-of-words' approach the LSTM model can take into account sentence structure. Where the BoW method considers only how many times a particular word occurs in tweet, the LSTM model can take into account word order through using a chain like structure of cell states.\n",
    "<br><br>\n",
    "![The repeating module in an LSTM contains four interacting layers.](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)\n",
    "<br>\n",
    "<center><b> The repeating module of a simple unrolled LSTM containing four interacting layers.</b></center>\n",
    "<br>\n",
    "As the network encounters new pieces of information the these layers decides how much information is carried forward or let through, and how much is added. Each LSTM cell takes an input vector and the hidden output vector of the previous cell and produces an output vector and the hidden output vector for the next cell. \n",
    "\n",
    "We will benchmark this LSTM model against our previous Bag of Words model and possibly also the LSTM run on non cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The first step towards building our model is to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, BatchNormalization, LSTM, Embedding, Reshape\n",
    "from keras.models import load_model, model_from_json\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import urllib\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/43034960/many-to-one-and-many-to-many-lstm-examples-in-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique words are:  5233\n",
      "The first review looks like this: \n",
      "[0, 35, 3, 55, 9, 5233, 6, 9, 20, 3166, 82, 3421, 100, 205, 264, 100, 5233, 436]\n"
     ]
    }
   ],
   "source": [
    "all_text = ' '.join(df.tweet.values)\n",
    "words = all_text.split()\n",
    "u_words = Counter(words).most_common()\n",
    "u_words = [word[0] for word in u_words if word[1]>5] # we will only consider words that have been used more than 5 times\n",
    "# create the dictionary\n",
    "word2num = dict(zip(u_words,range(len(u_words))))\n",
    "word2num['<Other>'] = len(u_words)\n",
    "num2word = dict(zip(word2num.values(), word2num.keys()))\n",
    "\n",
    "int_text = [[word2num[word] if word in word2num else len(u_words) for word in Review.split()] for Review in df.tweet.values]\n",
    "\n",
    "print('The number of unique words are: ', len(u_words))\n",
    "print('The first tweet looks like this: ')\n",
    "print(int_text[0][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([len(t) for t in int_text],50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a many to one LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of tweets greater than 500 in length is:  6\n",
      "The number of tweets less than 50 in length is:  31946\n"
     ]
    }
   ],
   "source": [
    "print('The number of tweets greater than 30 in length is: ', np.sum(np.array([len(t)>30 for t in int_text])))\n",
    "print('The number of tweets less than 30 in length is: ', np.sum(np.array([len(t)<30 for t in int_text])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You cannot pass differing lengths of sentences to the algorithm. Hence we shall prepad the sentence with <PAD>. Sequences less than 500 in length will be prepadded and sequences that are longer than 500 will be truncated. It is assumed that the sentiment of the review can be asserted from the first 500 words."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
