{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis: Classifying Hate Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal for this project is to build a model that will allow us to detect hate speech in tweets. In this case a tweet is considered to be hate speech if it contains racist or sexist remarks.\n",
    "\n",
    "All data for this project is sourced from: https://datahack.analyticsvidhya.com/contest/practice-problem-twitter-sentiment-analysis/. Inspiration and some code for this project has been taken from:\n",
    "- 'Stemming - Natural Language Processing With Python and NLTK p.3'- https://www.youtube.com/watch?v=yGKTphqxR9Q\n",
    "- https://datascienceplus.com/twitter-analysis-with-python/\n",
    "- Ultimate guide to deal with Text Data (using Python) – for Data Scientists & Engineers - https://www.analyticsvidhya.com/blog/2018/02/the-different-methods-deal-text-data-predictive-python/\n",
    "- 'Deep Learning tutorials in jupyter notebooks.' - https://github.com/sachinruk/deepschool.io\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the data & summarizing some basic features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin we read our training data into a dataframe and briefly explore the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0   @user when a father is dysfunctional and is s...\n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3      0                                bihday your majesty\n",
       "3   4      0  #model   i love u take with u all the time in ...\n",
       "4   5      0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv('./train_E6oV3lV.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31962, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is labelled training data. Each row is 3-tuple (id, label, tweet). A label of 1 means the tweet is hate speech, a label of 0 is a normal tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31957</th>\n",
       "      <td>31958</td>\n",
       "      <td>0</td>\n",
       "      <td>ate @user isz that youuu?ðððððð...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31958</th>\n",
       "      <td>31959</td>\n",
       "      <td>0</td>\n",
       "      <td>to see nina turner on the airwaves trying to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31959</th>\n",
       "      <td>31960</td>\n",
       "      <td>0</td>\n",
       "      <td>listening to sad songs on a monday morning otw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31960</th>\n",
       "      <td>31961</td>\n",
       "      <td>1</td>\n",
       "      <td>@user #sikh #temple vandalised in in #calgary,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31961</th>\n",
       "      <td>31962</td>\n",
       "      <td>0</td>\n",
       "      <td>thank you @user for you follow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  label                                              tweet\n",
       "31957  31958      0  ate @user isz that youuu?ðððððð...\n",
       "31958  31959      0    to see nina turner on the airwaves trying to...\n",
       "31959  31960      0  listening to sad songs on a monday morning otw...\n",
       "31960  31961      1  @user #sikh #temple vandalised in in #calgary,...\n",
       "31961  31962      0                   thank you @user for you follow  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many tweets are labelled as \"Hate Speech\" in our training data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    29720\n",
       "1     2242\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = ['Normal Tweets', 'Hate Speech']\n",
    "colors = ['b', 'r']\n",
    "sizes = df.label.value_counts()\n",
    "explode = (0, 0.17)\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "fig1.set_size_inches((6, 6))\n",
    "ax1.pie(sizes, shadow=True, explode=explode, labels=labels, startangle=90, autopct='%.1f%%', )\n",
    "plt.axis(\"equal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to be able to apply this binary classification to our test data set. We will ultimately measure the success of our model using the **F1 Score** described below:\n",
    "\n",
    "**True Positives (TP)** - These are the correctly predicted positive values which means that the value of actual class is yes and the value of predicted class is also yes.\n",
    "\n",
    "**True Negatives (TN)** - These are the correctly predicted negative values which means that the value of actual class is no and value of predicted class is also no.\n",
    "\n",
    "**False Positives (FP)** – When actual class is no and predicted class is yes.\n",
    "\n",
    "**False Negatives (FN)** – When actual class is yes but predicted class in no.\n",
    "\n",
    "**Precision** = TP/TP+FP\n",
    "\n",
    "**Recall** = TP/TP+FN\n",
    "\n",
    "**F1 Score** = 2*(Recall * Precision) / (Recall + Precision)\n",
    "\n",
    "https://datahack.analyticsvidhya.com/contest/practice-problem-twitter-sentiment-analysis/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will summarize some features of the data such as the number of words, number of characters and average word length for each tweet. These may prove useful as features in our model, for example shorter tweets may be more likely to contain hate speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean word count 16.048182216381953\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  word_count\n",
       "0   @user when a father is dysfunctional and is s...          21\n",
       "1  @user @user thanks for #lyft credit i can't us...          22\n",
       "2                                bihday your majesty           5\n",
       "3  #model   i love u take with u all the time in ...          17\n",
       "4             factsguide: society now    #motivation           8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['word_count'] = df['tweet'].apply(lambda x: len(str(x).split(\" \")))\n",
    "print('Mean word count', df.word_count.mean())\n",
    "df[['tweet','word_count']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean character count 84.73962830861649\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>char_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  char_count\n",
       "0   @user when a father is dysfunctional and is s...         102\n",
       "1  @user @user thanks for #lyft credit i can't us...         122\n",
       "2                                bihday your majesty          21\n",
       "3  #model   i love u take with u all the time in ...          86\n",
       "4             factsguide: society now    #motivation          39"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['char_count'] = df['tweet'].str.len() # includes spaces\n",
    "print('Mean character count', df.char_count.mean())\n",
    "df[['tweet','char_count']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and Preparing the Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step we will take to prepare our data for building a model is to transform all tweet text to be lower case. This is an important step since we will be counting the frequencies of words later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    @user when a father is dysfunctional and is so...\n",
       "1    @user @user thanks for #lyft credit i can't us...\n",
       "2                                  bihday your majesty\n",
       "3    #model i love u take with u all the time in ur...\n",
       "4                  factsguide: society now #motivation\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tweet = df.tweet.apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "df.tweet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also remove punctuation since it does not provide any valuable information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    user when a father is dysfunctional and is so ...\n",
       "1    user user thanks for lyft credit i cant use ca...\n",
       "2                                  bihday your majesty\n",
       "3    model i love u take with u all the time in urð...\n",
       "4                    factsguide society now motivation\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tweet = df.tweet.str.replace('[^\\w\\s]','')\n",
    "df.tweet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first feature we wish to extract for building our model is to count the frequencies of every word as they occur in each tweet. This is known as the 'bag-of-words' method in in Natural Language rocessing (NLP). We can think of the bag-of-words as representing each tweet as a multiset, or alternatively a vector. The `sklearn.feature_extraction.text` submodule provides an easy means of vectorizing our tweets. http://scikit-learn.org/stable/modules/feature_extraction.html#common-vectorizer-usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(31962, 45131)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "tf_vectorizer = CountVectorizer()\n",
    "tf = tf_vectorizer.fit_transform(df.tweet.values)\n",
    "tf_backup = tf\n",
    "\n",
    "print(type(tf))\n",
    "tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words:  45131\n",
      "First 10 word labels:  ['0000001', '00027', '001', '0035', '00h30', '01', '0115', '0161', '019', '01926889917']\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique words: ',len(tf_vectorizer.get_feature_names()))\n",
    "print('First 10 word labels: ', tf_vectorizer.get_feature_names()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the first 10 words in our 'bag' we can see that there is a lot of noise in the data. In order to extract useful features we would like to filter these from our data (known as stop words in NLP). We use the sklearn CountVectorizer() to filter out common English language words ('the', 'a', 'to', etc.) and any word appearing less than 5 times in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New number of unique words:  5975\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(31962, 5975)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_vectorizer = CountVectorizer(min_df=5,stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(df.tweet.values)\n",
    "print('New number of unique words: ',len(tf_vectorizer.get_feature_names()))\n",
    "print(type(tf))\n",
    "tf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have reduced the number of words in our vocabulary dictionary (the words we are counting the frequencies of for each tweet) from 45131 to 5975. Cleaning these meaningless words from the data will hopefully improve the accuracy of our model. Lets see if this extra step combined with the basic cleaning & preprocessing we have already done is will let us extract some useful features for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "# shuffle the data then split into two halves to cross-validate\n",
    "idx = np.random.permutation(len(df))\n",
    "X_train = tf[idx][:15981].todense()\n",
    "X_test = tf[idx][15981:].todense()\n",
    "print(type(tf))\n",
    "y_train = df.label.values[idx][:15981]\n",
    "y_test = df.label.values[idx][15981:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15981, 5975)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Keras neural network API for training the model with TensorFlow backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.regularizers import l2, l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 100)               597600    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 597,701\n",
      "Trainable params: 597,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=100, activation='relu', input_dim=tf.shape[1]))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adagrad', metrics=[\"binary_accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "15981/15981 [==============================] - 2s 121us/step - loss: 0.2091 - binary_accuracy: 0.9365\n",
      "Epoch 2/2\n",
      "15981/15981 [==============================] - 2s 106us/step - loss: 0.1130 - binary_accuracy: 0.9606\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7feda6ebbba8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=2, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras gives us two functions for making predictions with our model. `model.predict(...)` predicts the probability of a tweet belong to a particular class. `model.predict_class(...)` which gives 1 or 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute predictions for one half of our training data\n",
    "y_test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15981, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.05257289],\n",
       "       [0.00083026],\n",
       "       [0.16286801],\n",
       "       ...,\n",
       "       [0.07200381],\n",
       "       [0.00396138],\n",
       "       [0.02009284]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y_test_pred.shape)\n",
    "y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9559476878793567"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to 0/1 labels and count how many non-hate speech tweets were predicted\n",
    "y_test_pred[y_test_pred<0.5] = 0\n",
    "y_test_pred[y_test_pred>=0.5] = 1\n",
    "np.count_nonzero(y_test_pred==y_test[:,None])*1.0/len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that our original distribution for the test data was 93% normal and 7% hate speech. So this is a plausible distribution at least. Lets try a few manual test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.68364376]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_case = tf_vectorizer.transform([\"trump will build a wall\"])\n",
    "model.predict(test_case.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9767095]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_case = tf_vectorizer.transform([\"trump trump trump trump\"])\n",
    "model.predict(test_case.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00927515]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_case = tf_vectorizer.transform([\"can't wait to go to the beach this summer\"])\n",
    "model.predict(test_case.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25465617]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_case = tf_vectorizer.transform([\"I like pies\"])\n",
    "model.predict(test_case.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7405607]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_case = tf_vectorizer.transform([\"police brutality fuels race tensions\"])\n",
    "model.predict(test_case.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6001964]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_case = tf_vectorizer.transform([\"police brutally fueling racial tension\"])\n",
    "model.predict(test_case.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These test cases suggest our model has quite limited predictive power. This is hardly surprising given such a naive approach. For example the first & second tweet do not seem to convey any racist sentiment, although it seems to have some success at identifying words that are probably *associated* with racist/sexist tweets.\n",
    "\n",
    "In the the last test case the hate speech prediction changes by around 12% simply from including different word endings. Lets try a more advanced preprocessing technique known as **lemmatization**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization is a NLP technique similar to stemming. Both seek to reduce inflectional forms by reducing related words to their morphological root. e.g. [carrying, carried] -> carry. Stemming generally simply looks for suffixes to chop off, while lemmatization uses vocabularly and morpholigical analysis. https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus with lemmatization we should be able to further reduce the number of words in our 'bag-of-words' vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/trix/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    user when a father is dysfunctional and is so ...\n",
       "1    user user thanks for lyft credit i cant use ca...\n",
       "2                                  bihday your majesty\n",
       "3    model i love u take with u all the time in urð...\n",
       "4                    factsguide society now motivation\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from textblob import Word\n",
    "\n",
    "nltk.download('wordnet')\n",
    "df.tweet = df.tweet.apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "df.tweet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try retraining our model using the lemmatized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New number of unique words:  5649\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(31962, 5649)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_vectorizer = CountVectorizer(min_df=5,stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(df.tweet.values)\n",
    "print('New number of unique words: ',len(tf_vectorizer.get_feature_names()))\n",
    "print(type(tf))\n",
    "tf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using lemmatization we have decreased the number of words in our 'bag-of-words' from 5975 to 5649"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 100)               565000    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 565,101\n",
      "Trainable params: 565,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=100, activation='relu', input_dim=tf.shape[1]))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adagrad', metrics=[\"binary_accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "# shuffle the data then split into two halves to cross-validate\n",
    "idx = np.random.permutation(len(df))\n",
    "X_train = tf[idx][:15981].todense()\n",
    "X_test = tf[idx][15981:].todense()\n",
    "print(type(tf))\n",
    "y_train = df.label.values[idx][:15981]\n",
    "y_test = df.label.values[idx][15981:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "15981/15981 [==============================] - 2s 111us/step - loss: 0.2069 - binary_accuracy: 0.9357\n",
      "Epoch 2/2\n",
      "15981/15981 [==============================] - 2s 97us/step - loss: 0.1114 - binary_accuracy: 0.9601\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fed6d1b0c88>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=2, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute predictions for one half of our training data\n",
    "y_test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9577623427820537"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to 0/1 labels and count how many Normal tweets\n",
    "y_test_pred[y_test_pred<0.5] = 0\n",
    "y_test_pred[y_test_pred>=0.5] = 1\n",
    "np.count_nonzero(y_test_pred==y_test[:,None])*1.0/len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets run some playful test cases to see if we get any plausible results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.79193985]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_case = tf_vectorizer.transform([\"trump will build a wall\"])\n",
    "model.predict(test_case.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.970746]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_case = tf_vectorizer.transform([\"trump trump trump trump\"])\n",
    "model.predict(test_case.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00700677]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_case = tf_vectorizer.transform([\"can't wait to go to the beach this summer\"])\n",
    "model.predict(test_case.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.23759158]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_case = tf_vectorizer.transform([\"I like pies\"])\n",
    "model.predict(test_case.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6452479]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_case = tf_vectorizer.transform([\"police brutality fuels race tensions\"])\n",
    "model.predict(test_case.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.62352794]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_case = tf_vectorizer.transform([\"police brutally fueling racial tension\"])\n",
    "model.predict(test_case.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time the last 2 test cases using words with the same morpholigcal root had much more similar prediction scores, suggesting that the lemmatization was succesful and possibly helpful for analysing the sentiment as well. (Since these two tweets convey essentially the same sentiment we would expect their scores to be similar)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "By experimenting with various cleaning/preprocessing approaches then feeding them into a very basic model for sentiment analysis we can get some idea of whether these steps are helpful for feature extraction in this context. \n",
    "\n",
    "In the next stage of this project we will investigate improving our predictions with a more advanced model. We may improve on the 'bag-of-words' approach using Term Frequency - inverse Document Frequency (Tfidf). Or utilise a more sophisticated method such as a Recurrent Neural Network capable of taking into account word order. The preprocessing steps we have already carried out will most likely be useful for these approaches as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        user when a father is dysfunctional and is so ...\n",
       "1        user user thanks for lyft credit i cant use ca...\n",
       "2                                      bihday your majesty\n",
       "3        model i love u take with u all the time in urð...\n",
       "4                        factsguide society now motivation\n",
       "5        22 huge fan fare and big talking before they l...\n",
       "6        user camping tomorrow user user user user user...\n",
       "7        the next school year is the year for examsð ca...\n",
       "8        we won love the land allin cavs champion cleve...\n",
       "9                      user user welcome here im it so gr8\n",
       "10       â ireland consumer price index mom climbed fro...\n",
       "11       we are so selfish orlando standwithorlando pul...\n",
       "12           i get to see my daddy today 80days gettingfed\n",
       "13       user cnn call michigan middle school build the...\n",
       "14       no comment in australia opkillingbay seashephe...\n",
       "15             ouchjunior is angryðgot7 junior yugyoem omg\n",
       "16       i am thankful for having a paner thankful posi...\n",
       "17                                    retweet if you agree\n",
       "18       it friday ð smile all around via ig user user ...\n",
       "19       a we all know essential oil are not made of ch...\n",
       "20       euro2016 people blaming ha for conceded goal w...\n",
       "21       sad little dude badday coneofshame cat pissed ...\n",
       "22       product of the day happy man wine tool who it ...\n",
       "23               user user lumpy say i am a prove it lumpy\n",
       "24       user tgif ff to my gamedev indiedev indiegamed...\n",
       "25       beautiful sign by vendor 80 for 4500 upsideoff...\n",
       "26       user all smile when medium is ðð pressconferen...\n",
       "27       we had a great panel on the mediatization of t...\n",
       "28                              happy father day user ðððð\n",
       "29       50 people went to nightclub to have a good nig...\n",
       "                               ...                        \n",
       "31932                                    user thanks gemma\n",
       "31933    user judd is a amp homophobic freemilo milo fr...\n",
       "31934    lady banned from kentucky mall user jcpenny ke...\n",
       "31935    ugh im trying to enjoy my happy hour drink amp...\n",
       "31936    want to know how to live a life do more thing ...\n",
       "31937                                        love island ð\n",
       "31938    my fav actor vijaysethupathi my fav actress us...\n",
       "31939                    whew ð it a productive and friday\n",
       "31940                          user shes finally here user\n",
       "31941    passed first year of uni yay love pas unistude...\n",
       "31942    this week is flying by humpday wednesday kamp ...\n",
       "31943    user modeling photoshoot this friday yay model...\n",
       "31944    youre surrounded by people who love you even m...\n",
       "31945       feel like ððð dog summer hot help sun day more\n",
       "31946    user omfg im offended im a mailbox and im prou...\n",
       "31947    user user you dont have the ball to hashtag me...\n",
       "31948    make you ask yourself who am i then am i anybo...\n",
       "31949    hear one of my new song dont go katie ellie yo...\n",
       "31950    user you can try to tail u to stop butt were j...\n",
       "31951    ive just posted a new blog secondlife lonely neko\n",
       "31952                      user you went too far with user\n",
       "31953    good morning instagram shower water berlin ber...\n",
       "31954    holiday bull up you will dominate your bull an...\n",
       "31955    le than 2 week ð ðð¼ð¹ððµ user ibizabringitonm...\n",
       "31956    off fishing tomorrow user carnt wait first tim...\n",
       "31957                   ate user isz that youuuðððððððððâï\n",
       "31958    to see nina turner on the airwave trying to wr...\n",
       "31959    listening to sad song on a monday morning otw ...\n",
       "31960    user sikh temple vandalised in in calgary wso ...\n",
       "31961                        thank you user for you follow\n",
       "Name: tweet, Length: 31962, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       1.00      0.96      0.98     15442\n",
      "        1.0       0.44      0.89      0.59       539\n",
      "\n",
      "avg / total       0.98      0.96      0.96     15981\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(15981,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(np.round(model.predict(X_test)), y_test))\n",
    "y_test.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
